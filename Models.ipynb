{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16a657af",
   "metadata": {},
   "source": [
    "# Introducing the test models\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d78f969",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, cohen_kappa_score\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc3ab66",
   "metadata": {},
   "source": [
    "## 4.3 Supervised Learning Approach  \n",
    "(logistic regression, decision tree, Naive Bayes and K-nearest neighbor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def07d13",
   "metadata": {},
   "source": [
    "Multinomial Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ceca590d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 1000\n",
      "Feature count: 25\n",
      "Class distribution: Counter({np.str_('Standing'): 200, np.str_('Sitting'): 200, np.str_('Walking'): 200, np.str_('Running'): 200, np.str_('Stairs'): 200})\n",
      "\n",
      "Training samples: 700\n",
      "Testing samples: 300\n",
      "\n",
      "Training Multinomial Logistic Regression...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Masters\\Q1\\5ARE0_Data_analysis_and_learning_methods\\Assignment_1\\GitClone\\Data-Analysis-Assignment-1\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete.\n",
      "\n",
      "--- Model Evaluation (Multinomial Logistic Regression) ---\n",
      "Accuracy: 0.1833\n",
      "F1-Score (Macro): 0.1818\n",
      "Cohen's Kappa: -0.0208\n",
      "\n",
      "Confusion Matrix:\n",
      "          Walking  Running  Stairs  Sitting  Standing\n",
      "Walking        14        6      16        8        16\n",
      "Running        14       10       9       16        11\n",
      "Stairs         17       11      12       12         8\n",
      "Sitting        16       14      12       12         6\n",
      "Standing       12       21      10       10         7\n",
      "\n",
      "--- Feature Importance (Top Weights per Class) ---\n",
      "Top 5 most influential features for predicting Walking:\n",
      "Feature_16    0.183087\n",
      "Feature_12    0.181233\n",
      "Feature_21    0.139186\n",
      "Feature_10    0.137203\n",
      "Feature_14    0.123409\n",
      "Name: Walking, dtype: float64\n",
      "Top 5 most influential features for predicting Running:\n",
      "Feature_19    0.219202\n",
      "Feature_21    0.208253\n",
      "Feature_24    0.162814\n",
      "Feature_13    0.160326\n",
      "Feature_14    0.126607\n",
      "Name: Running, dtype: float64\n",
      "Top 5 most influential features for predicting Stairs:\n",
      "Feature_2     0.176742\n",
      "Feature_7     0.139907\n",
      "Feature_10    0.139435\n",
      "Feature_24    0.115670\n",
      "Feature_9     0.108829\n",
      "Name: Stairs, dtype: float64\n",
      "Top 5 most influential features for predicting Sitting:\n",
      "Feature_22    0.166094\n",
      "Feature_23    0.160064\n",
      "Feature_10    0.159907\n",
      "Feature_12    0.157264\n",
      "Feature_21    0.119660\n",
      "Name: Sitting, dtype: float64\n",
      "Top 5 most influential features for predicting Standing:\n",
      "Feature_23    0.218514\n",
      "Feature_18    0.207828\n",
      "Feature_25    0.185984\n",
      "Feature_8     0.161128\n",
      "Feature_19    0.144676\n",
      "Name: Standing, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# --- ASSUMED INPUT DATA STRUCTURE ---\n",
    "# X: Feature matrix (N_samples x N_features)\n",
    "# y: Target labels (N_samples,)\n",
    "# N_samples: Number of sliding windows (data points)\n",
    "# N_features: Number of features extracted per window (e.g., Mean, SD, RMS for each of the 9 axes)\n",
    "\n",
    "# Example: Replace with your actual loaded and engineered data\n",
    "# X = pd.DataFrame(data=..., columns=feature_names) \n",
    "# y = np.array(labels)\n",
    "# ------------------------------------\n",
    "\n",
    "# --- STEP 1: LOAD AND PREPARE DATA ---\n",
    "# This is a placeholder for your feature matrix and labels.\n",
    "# You must replace this with the actual output from your Feature Engineering step.\n",
    "# For a runnable example, we create dummy data:\n",
    "N_SAMPLES = 1000\n",
    "N_FEATURES = 25\n",
    "# 5 target activities: Walking, Running, Stairs, Standing, Sitting\n",
    "ACTIVITIES = ['Walking', 'Running', 'Stairs', 'Sitting', 'Standing'] \n",
    "N_CLASSES = len(ACTIVITIES)\n",
    "\n",
    "# Create dummy feature data (e.g., random data following a standard normal distribution)\n",
    "np.random.seed(33)\n",
    "X = np.random.randn(N_SAMPLES, N_FEATURES)\n",
    "\n",
    "# Create dummy labels (e.g., ensuring a somewhat balanced dataset)\n",
    "label_repetitions = N_SAMPLES // N_CLASSES\n",
    "y = np.repeat(ACTIVITIES, label_repetitions)[:N_SAMPLES]\n",
    "# Shuffle the labels to simulate a real-world mixed dataset\n",
    "np.random.shuffle(y) \n",
    "\n",
    "print(f\"Total samples: {len(X)}\")\n",
    "print(f\"Feature count: {X.shape[1]}\")\n",
    "print(f\"Class distribution: {Counter(y)}\")\n",
    "\n",
    "# --- STEP 2: SPLIT DATA ---\n",
    "# Split the dataset into training and testing sets (e.g., 70% train, 30% test)\n",
    "# Use stratification to ensure the activity distribution is maintained in both sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=33, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining samples: {len(X_train)}\")\n",
    "print(f\"Testing samples: {len(X_test)}\")\n",
    "\n",
    "# --- STEP 3: FEATURE SCALING (IMPORTANT FOR LOGISTIC REGRESSION) ---\n",
    "# Logistic Regression, like many models based on gradient descent, benefits from scaling.\n",
    "# We fit the scaler ONLY on the training data to prevent data leakage.\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# --- STEP 4: TRAIN MULTINOMIAL LOGISTIC REGRESSION MODEL ---\n",
    "\n",
    "# LogisticRegression is a suitable model for multi-class classification\n",
    "# when 'multi_class' is set to 'multinomial' (or 'auto', which defaults to 'multinomial'\n",
    "# if the solver supports it, and 'ovr' otherwise).\n",
    "# 'solver'='lbfgs' is a good default choice for small to medium datasets and multinomial loss.\n",
    "\n",
    "# Hyperparameter Choice:\n",
    "# C (Inverse of regularization strength): A lower C means stronger regularization.\n",
    "# C=1.0 is a common starting point. You may need to tune this.\n",
    "\n",
    "mlr_model = LogisticRegression(\n",
    "    penalty='l2',            # L2 regularization\n",
    "    C=1.0,                   # Regularization strength\n",
    "    multi_class='multinomial', # Set for multi-class classification\n",
    "    solver='lbfgs',          # Optimization algorithm\n",
    "    max_iter=500,            # Increase if convergence warning appears\n",
    "    random_state=33,\n",
    "    n_jobs=-1                # Use all processors for faster training\n",
    ")\n",
    "\n",
    "print(\"\\nTraining Multinomial Logistic Regression...\")\n",
    "mlr_model.fit(X_train_scaled, y_train)\n",
    "print(\"Training complete.\")\n",
    "\n",
    "# --- STEP 5: PREDICT AND EVALUATE MODEL PERFORMANCE ---\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = mlr_model.predict(X_test_scaled)\n",
    "\n",
    "# [cite_start]Calculate Evaluation Metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "kappa = cohen_kappa_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred, labels=ACTIVITIES)\n",
    "\n",
    "# Display Results\n",
    "print(\"\\n--- Model Evaluation (Multinomial Logistic Regression) ---\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1-Score (Macro): {f1_macro:.4f}\")\n",
    "print(f\"Cohen's Kappa: {kappa:.4f}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(pd.DataFrame(conf_matrix, index=ACTIVITIES, columns=ACTIVITIES))\n",
    "\n",
    "# --- STEP 6: MODEL INTERPRETATION (Feature Weights) ---\n",
    "\n",
    "# The coefficients (weights) give insight into feature importance.\n",
    "# Note: For multi-class, there is one set of coefficients per class.\n",
    "# The absolute magnitude of a coefficient indicates its influence on that specific class.\n",
    "print(\"\\n--- Feature Importance (Top Weights per Class) ---\")\n",
    "\n",
    "# Placeholder for your actual feature names\n",
    "feature_names = [f'Feature_{i+1}' for i in range(N_FEATURES)]\n",
    "coef_df = pd.DataFrame(mlr_model.coef_, columns=feature_names, index=ACTIVITIES)\n",
    "\n",
    "for activity in ACTIVITIES:\n",
    "    weights = coef_df.loc[activity].abs().sort_values(ascending=False)\n",
    "    print(f\"Top 5 most influential features for predicting {activity}:\")\n",
    "    print(weights.head(5))\n",
    "\n",
    "# You should use this interpretation to help answer Research Question 1\n",
    "# by relating the highest-weighted features to your sensor analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2addc304",
   "metadata": {},
   "source": [
    "## 4.4 Unsupervised Learning Approach  \n",
    "(hard) k-means, fuzzy C-means and Gaussian mixture models. Make use of PCA\n",
    "for visualization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
